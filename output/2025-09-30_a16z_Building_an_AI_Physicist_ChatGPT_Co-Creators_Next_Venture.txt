================================================================================
YOUTUBE VIDEO EXTRACTION
================================================================================
VIDEO_ID: 5FoWFeJCa2A
URL: https://www.youtube.com/watch?v=5FoWFeJCa2A
TITLE: Building an AI Physicist: ChatGPT Co-Creator’s Next Venture
CHANNEL: a16z
PUBLISHED: 2025-09-30
DURATION: 51m 49s
VIEWS: 12,953
LIKES: 0
COMMENTS: 24
TAGS: a16z, andreessen horowitz

DESCRIPTION:
----------------------------------------
Scaling laws took us from GPT-1 to GPT-5 Pro. But in order to crack physics, we’ll need a different approach.

In this episode, a16z General Partner Anjney Midha talks to Liam Fedus, former VP of post-training research and co-creator of ChatGPT at OpenAI, and Ekin Dogus Cubuk, former head of materials science and chemistry research at Google DeepMind, on their new startup Periodic Labs and their plan to automate discovery in the hard sciences.

00:00 Introduction
02:17 The Role of LLMs in Physics and Chemistry Research
03:53 What is Periodic Labs?
05:25 The Importance of Experimentation
07:44 Challenges and Goals in Physics Research
14:45 Building the Team
17:29 Scaling Laws and Physical Verification
22:36 Focus on Superconductivity
25:33 Creating a Repeatable Process for ML Systems
26:08 Balancing Commercial Viability and Scientific Goals
27:39 Periodic's Mission and Industry Applications
28:49 Integrating Diverse Expertise in the Team
29:52 Teaching LLMs to Reason in Physics and Chem
... [truncated]

SUBTITLE AVAILABILITY:
----------------------------------------
Total languages: 157
English available: True (auto)

EXTRACTED CAPTIONS:
----------------------------------------
Method: easysub-api
Language: en
Word count: 9,556

TRANSCRIPT:
----------------------------------------
ultimately sciences driven against experiment in the real world. And so that's what we're doing with periodic labs. We're taking these precursor technologies and we're saying okay if you care about advancing science we need to have experiment in the loop. The applications of building an AI physicist for lack of a better word that can design the real world are so broad. You can apply them to advanced manufacturing. You can apply them to material science to chemistry any process where there's R&D with the physical world required. It seems like we'll benefit from breakthroughs that periodic is working on. >> For example, if we could find a 200 Kelvin superconductor even before we make any product with it. To be able to see such quantum effects on at such high temperatures, I think would be such an update to people's view of how they see the universe. So Liam, you were the co-creator of Chad Gupt. You were running some of the physics teams at Deep Mind. Let's talk about how you guys met and what was the moment where you realized that you guys had to leave both of those labs to start periodic. >> I believe we met eight years ago at Google brain flipping over a large tire. >> Y >> at the Google >> they got to give us more on that story. Um, so Google Rails, uh, was one of the gyms at Google at the Google facilities and I I think that's where Doge and I met and it was just this massive tire that like a single person basically can't flip single like by themselves. And so Doge was trying to flip it and he like pulled me over. He's like, I think the two of us could do it. So, >> and why were you trying to flip this tire? >> You know, why not? Um, but yeah, I I tried doing it. I couldn't do it. And then I was like, "Who's the strongest person I can find?" And it was either Barrett or Liam. And I Liam and it worked. We did flip it. So, >> And was that um the moment where you guys both realized you had physics backgrounds? How did that happen? What h how did you go from from flipping tires to flipping experiments? >> Yeah. I mean, so I don't know if De remembers this, but we would catch up, you know, over the years. And we would often end up talking about either quantum mechanics or like superconductivity. This was like very common, but I never thought they would end up working on physics together. Um, so Liam was working on LLMs and they were going really well. Um, and I was not using LLMs, but I was noticing that LM are becoming more and more impactful in my work. So one way it was becoming impactful is when I was trying to remember some things about chemistry, physics, I could just talk to the chatbot and actually learn a lot of stuff I forgot. Another way was of course coding. like we were writing simulations and the LM was so helpful in writing these simulations for us. So then the question was like can we use LM's kind of more as a first class citizen in the physics research. >> Yeah. And I think kind of leading up to this decision to leave um Doge and I were just you know connecting and talking about these different tech trees. We're looking at the improvements on language models on reasoning. We're seeing what high compute reinforcement learning could do. And on the material science side, we're seeing scaling laws within physics, within chemistry, uh both with respect to simulations, with respect to experiment. And it's like the same kind of principles at play in ML. And I think to both of us and to a lot of people in the field, the goal of this technology is accelerate science, accelerate physical R&D. you know, chat bots was like a great milestone along the way, but we really want to see technology out in the world >> and we felt like this was just the right place to begin. Uh, physics is very verifiable. It's it's a great reward function, fairly fast iteration loop. You have simulators for large classes of physical systems. And we felt like in order to create this AI scientist, this is like the beginning of of this path. So built that conviction and decided to found periodic. >> Well, let's take a second to talk about what periodic is and what does it do. So periodic labs is a frontier AI research lab that's trying to use LLMs to advance physics and chemistry. Um we feel like having experiment in the loop tightly coupled with simulations and LMS is extremely important. So we're building up a lab that will generate high throughput, high quality data and we will use LLMs and simulations in conjunction with the experiments to try to iterate. Science by its nature is a iterative um direction and we feel like LLM's using all these tools that are available to humans can do a great job in accelerating physical R&D. I'd say the objective is let's replace the reward function from math graders and code graders that we're using today. So like math graders, you know, to give an example, uh you have a prompt, what is 2 plus 2? You know, the ground truth is four. You can put a lot of optimization pressure against problems like that that are programmatically checkable. And what we're doing and by having the lab is we create a a physically grounded reward function that becomes the basis on which we're optimizing against. And so if a simulator has some deficiencies or some issues, we always error correct because for us the ground truth is the experiment like the RL environment. Nature like is our RL environment in in our setting. Let's just take a second for folks who might not be familiar to explain what you guys mean by a lab that will verify RL in the real world. Can you talk a little bit about how experiments work? How does how do how are AI models trained today and how is it how are those different from how they're going to be trained and and developed and post-trained and deployed at periodic and it might be helpful to talk about how you created chatpt. So chatbt originally um the technology evolved very rapidly over the over the last few years. When we were first creating it, it was a very standard RHF pipeline. So you have a pre-trained model and it's sort of like this like raw substrate and what you're trying to do is take this autocomp completion model and turn it into something useful. The way we did it at that point was we would have supervised data. So given some input, we would say this is a desired output. So if we're trying to get it to act as an assistant, you know, we create some tupils like that, then you run reinforcement learning, but now you're learning against a a reward function that's trained against human preferences. So humans will say, well, given this input, I would prefer completion A to completion B. And you do that over and over again. And you can create a reward function that can then be optimized against. That is sort of the basis of how we created chatbt. But then there's a huge gap between the original model and what we have today. >> And I think part of that is reasoning, but also part of that is just much better, more precise reward functions. >> So the reward functions that we were using originally couldn't determine whether you were mathematically correct or not. So early versions of chaptt were mathematically not particularly strong and it sort of results from the reward function. What did you optimize against? You the reward function basically encoded be a friendly assistant try to help people get to their thing. But it had no sense of is this mathematically correct or not? Is this code valid or not? And we made huge advances over the correctness of reward functions. >> Um but this is all digital. uh we're creating tasks based on the internet, textbooks, papers, and this is great. This lays a foundation, but ultimately science is driven against experiment in the real world. And so that's what we're doing with periodic labs. We're taking these precursor technologies and we're saying, okay, if you care about advancing science, we need to have experiment in the loop and that becomes our reward function for our agents. So, as Doge was saying, our agents are doing the same type of things you would use for coding or to help answer a query, but now instead of just giving tools like here's Python, here's a browser, now we have tools like quantum mechanics, so simulate different systems, but ultimately we're going to a lab and then that becomes like the basis of what is the the system optimizing against. >> So, >> so that's sort of just like the natural like end state of these systems. People in AI often say lab often what they're referring to is quite different from what you guys mean by a lab to what what's the difference >> that's right so as D mentioned so far the LMS have gotten really good at logic and math there's like verifiable rewards what is like the next frontier in terms of you know inquiry after logic and math I'd say is physics and then when you say physics there are different energy scales so there's astrophysics studying galaxies there's fusion nuclear physics but then there's the energy scale of physics that's more relevant to our life and that's the quantum mechanics like shinger's equation this is where you know biology happens chemistry around us happens materials happen so we felt like our first lab should be basically probing that quantum mechanical energy scale um and for us that would be physics at the level of solid state physics material science and chemistry uh the kind of one of the more fundamental ways of making things around us is powder synthesis so you take powders of existing materials you mix them and you heat them up to certain temperature and it becomes a new material. Uh so that's one of our labs. We're going to have a powder synthesis lab and turns out this is one of those methods where robots can do it like very cheap simple methods. Um I don't know if you saw this uh coffee making robot in the SF airport. You know a robot that's basically at that level can mix powders and put it in a furnace. Um and that's a very rich field. So you can actually using that method discover new superconductors, magnets, you know, all kinds of materials that are very important for technologies around us. But at the core of it, it's just quantum mechanics. Uh and we feel like teaching these LLMs to be foundation models, but for quantum mechanics will be the next frontier for LMS. >> Why haven't the models that are currently out in the world and deployed able to do this? >> Great question. I think like as Le mentioned earlier, science is by its nature iterative, right? Like even the smartest humans tried many times before they discovered the things they discovered. And I think maybe this is one of the confusing points about LLMs. LM can be very smart, but if they're not iterating on science, they won't discover science. You know, to be honest, humans won't either. Like you put a human in a room without any chance to iterate on something, they won't discover anything important. So we feel like the important thing to teach these LLMs is the method of scientific inquiry. So you do simulations, you do theoretical calculations, you do experiments, you get results and the results are probably incorrect or not what you want at first, but you iterate on it. >> And we feel like that hasn't been done yet. So this is what we want to do, but we feel like you have to do it with the real physics, not just the simulation. So this is why we have our own lab where the LLM will have the opportunity to iterate on its understanding of quantum mechanics. Fundamentally, machine learning models are good at what you train them to do, >> and that's sort of like the nature of it. And so, if a model is acting badly, you're like, well, did you train it to do that task? Um, kind of building on Do's point, there's sort of like an epistemic uncertainty, this like reducible uncertainty that you aren't really building or collapsing unless you're actually running an experiment. So for instance uh one of the engineers on our team was looking at a reported property of some like some physical property in the literature and it spanned many orders of magnitude. >> So if I train a system on that these systems aren't magic the best they can do is replicate that distribution but it's really no closer to a deeper understanding of the universe physics chemistry. Um then another point is it's very uncommon to publish negative results. All of the results are basically positive and a ne a valid negative result is very valuable. A negative result could be discarded because like well it was sloppy science but there are are valid negative results and that's a learning signal and this is something that our lab will produce as well. So I think these three things there's just like noisy data no negative results and you need the ability to act in order to actually do science which is an iterative endeavor. Those are like the core thesis of why we need a lab. And what might be the core way to measure if periodics progress against that goal in your guys' minds? >> One simple one is let's say high temperature superconductivity. What is the highest temperature superconductor we synthesized? Uh today the best number for ambient pressure is 135 Kelvin or so. So we'll know very easily if you're doing well if we can go beyond that number. Um so that's pretty fundamental. On the more applied side, you know, there's a processing of materials and its effect on the materials properties. So, we can just measure these properties directly. They say it's the ductility, it's the toughness, strength of the prop material. And as we measure it, the LM will get very clear signal. It's hard to hack, you know, unless unlike these other LLM training techniques. It's like really what you see in real life is the signal this going to the LLM. >> Yeah. Effectively, it's like do you can you design the world around you? So you're like, I need something with this property. Can this system discover and produce that both from like a fundamental scientific discovery perspective, but also in industry? So like someone's working in space or defense or semiconductors and like yeah, we're having these issues. We're trying to uh achieve this property of this material or this layer. Can the system accelerate um the development of those technologies? So it's it's it's very grounded. um that's how we'll know it's working. It feels like the applications of solving building an AI physicist for lack of a better word that can design the real world are so broad. You can apply them to advanced manufacturing. You can apply them to material science to chemistry to all anything that inter any process where there's R&D with the physical world required it seems like will benefit from breakthroughs that periodic is working on. Why hasn't it been done before? And what is it about this moment in history that makes it the right time to attack this problem? >> Maybe one comment is difficult. >> What makes it so difficult? >> I mean, I think part of it is the team. >> So, in our view, this has been enabled by frontier technology in the last couple of years. And so Doge and I have been so focused on basically putting together like this N of one team like these group of physicists, chemists, uh um simulation experts and some of the best machine learning researchers in the world have never been part of one concerted effort and we feel in order to actually achieve this you need all these expertise you need these these pillars to do this. So when you guys went about designing the team you know after you left open deep mind what was the primary heristic that you that you used to guide yourself in figuring out who you wanted on the team? So in terms of expertise we wanted to have a LM expertise covered the experimental expertise and simulation and for each of these we wanted to have basically world-class talent um and of course for each team there's actually a lot of sub teams like it's like a fractal right like expertise is very fractal like so for the experimental side we want to cover solid state chemistry solid state physics automation and kind of the more facilities like the more um operational aspects of experiment ments on the simulation side there's the more kind of theoretical physics parts there's the more kind of coding aspects of simulations and on the LLM side of course there's mid-training RL infra and yeah for each of these we try to get basically the best people who have innovated in these like subpillars the technology that we think is necessary to do it has really just emerged in the last couple of years and this data isn't like on a Reddit forum or something like you you need to actually go produce experimental data, simulation data. It's siloed across all of these advanced industries and many of them while there's a desire, they may not have knowledge of, you know, some of the most recent techniques that's been driving this this recent wave in AI. There was a moment in time when models like or or papers like the GPT3 paper for example that you know set of language models or few shot learners and proposed the idea of scaling laws and then there was a follow-up paper if you guys remember from openi that um was called I think scaling laws for generative modeling um that just showed that as long as you just kept throwing you scaled up the amount of compute and data in the right combination um you could very predictably improve the performance of of these models. And the theory was that if you just kept doing that, you know, in add at at Infinitum, um there would be a bunch of emerging capabilities. These models would be able to reason about all kinds of problems out of domain, out of distribution. Um is wouldn't that argue h how would you square the circle with that school of thought that um that you know naively the current pre-training and post-training uh sort of pipelines at most of the frontier labs won't just eventually crack physics as well. Why why is is this idea of physical verification um so necessary and are is that school of sort of reasoning wrong? >> Yeah. Um, excellent question. Uh, scaling laws empirically seem to continue to hold. So that's not in question, but I think there's a question of what is this yaxis? >> And that test distribution is very different from like what we're talking about. >> Um, that test distribution, let's say you're pre-training on the internet, might be, you know, a representative set from the internet and you will have these sort of predictable scaling properties. But that's not going to capture that you you have a very different set of scaling properties with respect to different distributions. So I try to make this a little bit more concrete. Let's say hypothetically we're training a coding model and we have unit tests to provide some reward signal. So the model writes some PR. We check that the unit tests go from failing to passing and we say this was successful. We're going to reinforce these things. You might say you start optimizing this and now the system is becoming ever more capable of writing code for its own development and you have this acceleration. You have this kind of takeoff scenario. Um code is one of the most promising areas for this because there's abundance of data online. You have this feedback loop where the system itself can begin to improve itself. And it's it's a very promising technique and we're all seeing the benefits of um advanced coding models and it's accelerating quickly. However, that model is not going to then cure cancer. The knowledge simply doesn't exist. It it doesn't you need to optimize against the distribution you care about. So that model while it's going to be a very valuable tool as a software engineer, it may help a cancer researcher do their analysis, it simply doesn't have the data, the knowledge or the expertise iterating against that environment. >> And I think that's just sort of like the fundamental belief we have. >> Yeah. I mean so actually Lee and I worked on this a bit when we're looking at the scaling laws for uh vision models and you know this also came up a lot in the clip paper from openai like the in domain generalization and the out of domain generalization are monotonically correlated but it's not linear necessarily and so what that means is you can keep improving your model and it will improve as the power law in domain >> and for auto domain tasks by which I mean as Liam said the things that you're trying to do that's a bit different than what's in your training set will also improve as a power law but the slope of that power law may not be good enough. So like you might need to you know spend centuries before you get to the result you want. We saw this in the non paper for example we published a paper where we saw that as you increase the size of your training set the iid performance the in domain performance improves as a paral law out domain performance also improves as a paral law but depending on what the out domain is like how far you are from the training distribution that parallel might have such a small slope that is basically useless >> um so this is one of the reasons we feel like the best way to make progress is to make your target as close to your in domain training training set as possible and the best way of doing this is to basically iterate on changing your training set to be more like what you want to do. Uh so this is one answer. The other one is actually maybe even simpler. Um the experimental data we want actually doesn't exist. So for example, if you look at uh like you want to say learn on the experimental data in literature for synthesis. Turns out the formation enthalpy labels which is like the energy it takes to basically assemble the atoms in the shape you want um is so high that if you train a machine learning model on it it's not predictive enough to predict the next one. Uh so and one of the reasons for this is as Liam mentioned people don't usually publish negative results and negative results are usually very context dependent. So what's a negative result for someone might be positive if they do things differently. So um yeah, so not only is there this domain shift problem where what you're trying to do might be different than your training set. So the power law won't have the large enough slope you want but the other problem is for some of these things we want to do there's no data for it. Uh for example for superc conductivity there's a lot of data sets you can look at but the noise floor on them is so high that training on them usually doesn't help. Doge, me, the entire team are deep believers in scaling up and scaling laws, but it's just do a beline for the thing you care about. Um, and in our case, we care about advancing science, advancing physical R&D. Um, that's that's sort of like the thesis. Is there a tension between being super bitter less impilled and just throwing more compute at the problem and the I guess domain specific pipelines that the lab you guys just described will have to focus on in the case of periodic I think you mentioned the first bel lines you guys are making are towards superc conductivity and magnetism right what is it about those domains that make them good candidates for the first um for the first few pipelines that per's working on and why are they just are they pit stops along the way to an AI physicist that generalizes across all kinds of domains or is there a danger of them being essentially offramps um that don't result in sort of a the the AI sort of scientific super intelligence that that is the north star for what you guys are doing. >> Yeah. Like I feel like for example the high temperature superc conductivity goal is actually a goal that has so many sub goals in it. It's a bit like when Deep Mind and Open AAI started and said we're going to do AGI, but what they meant was they had to do so many things before they got to these cool results. Like for us, if you want to get a high temperature superconductor, we probably need to get good at autonomous synthesis, autonomous characterization. We need to get good at um characterizing different aspects of the material, uh using the LLM to run the simulations correctly. So it's a northstar and there's so many goals in the on the way that would be very I think impactful for the community. >> Um that's one reason. Another reason is I feel like high temperature superconductivity is such a fundamentally interesting question. For example, if we could find the 200 Kelvin superconductor even before we make any product with it. That in itself says so much about the universe that we didn't know yet. you know to be able to see such quantum effects on at such high temperatures I think would be such an update to people's view of how they see the universe so we feel like it'll be really impactful for humanity even before we make a product out of it I think that's one of the reasons a technical reason also is superc conductivity is a phase transition so it's pretty robust to some of these details that we cannot simulate yet so for example when you make the material the superconducting temperature usually is more dominated by its kind of crystal fundamental property then like defects or microructure whereas there are certain other materials properties where even if the crystal has the property you want there's so many other factors that you cannot simulate that would prevent you from seeing that property so superconductivity has this like nice uh philosophical uh upside to it has this technical upside to it um and it like really rallies both the physicists like there are people who studied physics for 40 years and really excited about super conductivity and there are people who've never studied physics But very excited about for conductivity. It's like quite rare to find a topic that unites the whole team. >> Yeah. I mean it's like D said it's like we in order to do this there's so many foundational pieces to solve and our tactic is in order to actually get to this goal of AI scientist. You need to make contact do the full loop somewhere. If you say you're doing this in just like very vague terms, you sort of just end up back on archive papers and textbooks. And so it's it's really important for us to do the loop, but then create this repeatable process like how do you go from subdomain to subdomain? And there's really interesting questions about how well do the ML systems generalize between these things. Um what is the generalization of a system between like super connectivity data to magnetism data for instance? And maybe that looks very different than its ability to generalize to fluid mechanics. Um, and I think there's like fundamental arguments to make there. Um, but the goal is create this repeatable system, prove it, and then just go through the different domains that way. >> So I I can see the argument for why um cracking room temperature superconductivity from an experimental basis is is extraordinary valuable for humanity. But you guys are building a startup. And um to use an analogy for why you need to have a clear um medium-term path or short a medium-term path along the way to a northstar that is both commercially viable and net positive to to society. What we've seen for example with other frontier labs that are working on automating white collar work or or software knowledge work is that you know there's this northstar of an AI researcher um but that along the way there were a bunch of sub goals and so on um but a concrete kind of application that opened up a ton of commercial value and and benefits for users on the way to that AI researcher was the idea of of AI programming right software engineering has become uh probably the first major domain and that that's that's caused people to really update their priors about how uh useful AI models are beyond kind of consumer applications and in terms of productivity their impact been extraordinary just in a few short months. So if the traditional frontier labs as northstar was an AI researcher and the the path along the way to get there was programming AI programming what is that for periodic basically co-pilots for engineers researchers in advanced industries so maybe perhaps just being in Silicon Valley we you know we really think about like computer oriented work everything is digital everything is bits but there's so many industries Like we were kind of talking about a few like space defense semiconductors where they're dealing with iteration of materials of physics and that's part of their workflow like how are they designing these new technologies these new devices and in the absence of data in the absence of good systems they don't really have particularly good tools >> that is our opportunity and these are massive R&D budgets so uh yeah while high temp super connectivity is a great north star. We very much understand that technology and capital are intertwined. Um, we're going to be able to maximally accelerate science if this is a wildly successful commercial entity. And to do so, we want to accelerate advanced manufacturing in all these different industries. Become like an intelligence layer in these for for all these teams to accelerate their workflow and start reducing their iteration time, get them to better solutions more quickly, accelerate their researchers and their engineers. Let let's click a little bit deeper on the that in practice sort of a day in the life of a periodic uh team member where let's say half the team is this roughly right about half the team are ML scientists with machine learning backgrounds and the remaining half are physical scientists with physics or chemistry backgrounds um how do you start by uniting the cultures right how do you take somebody whose primary career so far in work has been experiments in a lab in in wet labs doing physics and chemistry um and give them an intuition for ML and vice versa because you know you guys are both physicists who then had the the the career trajectory where you also had the chance to be at at at Frontier AI labs and we're were part of training systems that are now considered sort of landmark hallmark machine learning systems um like Jack GPT like Gnome but for others who might be coming from one domain how do you get the team to build an intuition for the other >> yeah so this is a great question and we feel like it's actually crucial for us to make sure these teams work very closely with each other. Um, so one of the things we're seeing is the physics and the chemists need to figure out how to teach the LLM how to reason about these things because I think the frontier AI labs have figured out how to train them on math and logic but not yet on physics, chemistry. So one thing we're seeing that's been really I think productive is the physicists and chemists are thinking about what are the steps we should include in the mid training in the RL training that will teach the LLM how to reason correctly about quantum mechanics how to reason correctly about these physical systems. Um another one of course is the LLM researchers are learning quite a bit about the physics the simulation tools the goals. Uh so they've been working together really well. uh we have weekly teaching sessions where the LLM researchers teach you know how the RL loops work, how the data cleaning works and then the physicists and chemists are teaching about different aspects of the science um the history of science that's also very important. Uh so we feel like that's been going really well and you know one way of looking at this is the things we have to teach the LLM to be able to discover say a superconductor include being able to read the literature really well like read all the papers the textbooks find the relevant parts and then being able to run simulations theoretical calculations and then take action run experiments you know we feel like this is quite similar to the physical R&D researchers in these companies they have to read the literature read maybe internal documents or external documents and then run simulations, run theoretical calculations and then actually attempt the thing experimentally, learn from that. So we feel like all the progress we're making towards our internal superconductivity or physics goals actually is making our LLMs much better at serving our customers who are doing very similar workflows. Yeah, I think just culture, no stupid questions can ask just like the dumbest like physics question, the the dumbest ML question. And I mean there's a few faculty as part of our company and they're actually excellent teachers. >> Um so I mean these like learning sessions have been really fantastic and another thing I noticed is uh computer scientists often think in terms of like APIs. So scientists will say something and they're always trying to map it. You're like okay well what's the input? What's the output? what's the target, how do I map that back, what are and it's it's always just like this translation and I think we also have built up as part of the team there's there's people like on these different edges so like if you have uh a simplex of like you know pure ML LLM pure experimentalist pure simulation um there's people who like kind of live in this inside as well and so they've been like excellent bridges for translating between between these different groups of people so it's like active learning to like learn the other spaces creating APIs and then these kind of bridge connector peoples. Um I think Doge being an excellent example of that. >> Is it a requirement for somebody who wants to join periodic to have to have an advanced degree in physics or chemistry? >> Absolutely not. You know, one of the jokes we were making is who was the NBA player who was saying that I'm much closer to LeBron James than you are to me. Look, we were saying the opposite of that to candidates because the amount that even our best physicist doesn't know about physics is much bigger than the amount that they know about physics. So for this new candidate, even if they have no background in physics, how much they have to learn about what we're trying to do is actually not that different than how much the best physicist has to learn because there's so much chemistry to learn, so much material science to learn. And I think this is one of the interesting aspects of science today. You know in the past in 1800s there were these physicists that could do so many different things at the frontier. Today we've reached a point where our intellectual knowledge is so large that a leading thinker can usually only advance in one very specific field and maybe this is actually holding us back because say to discover an amazing superconductor as we keep going back to this example you have to know so much about chemistry physics synthesis characterization and unfortunately I don't think any human knows enough about all of these so we have to collaborate so I think our team is kind of like a small example of this where we have as Liam said like a lot of different points in that simplex and for any person they have so much to learn but that's true for basically every other scientist uh so for example I supposedly come from the physics side of it but I've been learning so much more physics because we now have people from different areas of chemistry in the team different areas of physics and I think it's true for LLM researchers as well I mean they come in there are aspects of LLM that they probably didn't know until they started working with other research in our team. So, I think it's a great and it's like a small example of what we're trying to do with the LLM because we're trying to teach this LLM all these different things that we're learning as researchers. >> It's like a really fun experience, I think. Yeah. >> And what are you finding makes a great researcher at Periodic that's different from what might make a great researcher at OpenAI or Anthropic or Deep Mind? >> I would say there's very high overlap. Um, but probably one of the biggest determinants is do you care about this mission? >> Is accelerating science is to you is that like the big goal and I think looking at the treat the team right now it's just incredibly missiondriven set of folks who are like yeah this is the northstar let's do that. Um, if someone really wants to improve some Mega Corps products, yeah, you'd probably be better off at that Mega Corp in iterating and improving their products. But if you care about scientific discovery, I think Periodic Labs is the best place to do that. >> How big is the team today? >> We're roughly 30, I believe. >> Yeah. And as you think about taking a lot of the research that's going on at the company and deploying that out in the real world, the kinds of customers that we've talked about, space, defense, advanced manufacturing, these are these are mission critical industries that are known for being um, you know, essential to whatever part of the economy they're part of, but often they're not the most they're not the fastest to adopt new technology. Um, how do you think about deploying the kinds of frontier agents that we've talked about that are great at science, great at physics, um, in companies or organizations that might not be anywhere close to as sophisticated as you are um, in AI or ML? Is is there do you have a working thesis for how to make sure that the arc of progress is not bottlenecked on deployment? Uh it sounds like you have a fairly good thesis on how to unblock the arc of scientific progress on the research side, but when it comes to deployment, what might be a working theory that you guys are optimistic about that would help get um the systems that periodic is building out into the real world? >> Well, maybe one thing that we've noticed in in our conversations with all these companies is they all are looking for their AI strategy. um they understand that like the techn is shifting really quickly and they're looking at how they're doing their work and it's not changing as quickly as they think it should be. >> Um some industries also are losing like kind of key expertise uh in different fields and they're like losing these like senior engineers, senior researchers and they're like okay how do we like preserve that? Um but one thesis is understand it's you know kind of thinking about these like APIs and thinking about what are the evaluations what are the biggest bottlenecks for these companies um looking at some of the problems they face and we can map that to our systems and we say well we think we can dramatically accelerate this and so it's not coming in and saying hey we're going to transform your fab line on day one we're going to transform how you're doing everything forget everything it's like no we're we're going to solve a really critical problem, well scoped, very clear evaluations. You kind of co-draft that with them and just show them like how powerful this technology can be when you optimize against the thing you care about. >> Um, so you know, nothing uh particularly like uh surprising here, but you know, sort of like a land and expand type method as as you might expect, >> but really looking for who are the biggest promoters um within that company. What are the biggest problems? Make sure you're solving a very real thing for them and intersect that with where is our technical capability the highest. >> You know, you were on a call this morning with one of the customers in your pipeline. We don't we don't need to name who, but what what was some of the things you heard as their as their most urgent problems that they'd like for periodic to solve. >> Uh so one of them was simulations. You know, they spend a lot of time training people on some of these simulations they need to use is critical for their development. Um and being able to automate those simulations I think would be quite enabling. Uh the design process um and then kind of like some of the small things like matching the formats being able to feed you know the simulation results into the design pipeline. All of these seem quite important and then being able to treat the data together in the same place. Uh what else? >> Well I think there's a really fundamental question. So a lot of these companies will rely on retrieval. So that's sort of like a super lightweight thing. Someone shows up with a neural net and they're like great, we'll just retrieve over all of your data and then that's your solution. However, as we've seen with things like chatbt and other things, it's when you pre-train on the data. When you actually encode the knowledge into the weights, it's not just a retrieval system, you have a richer, deeper understanding of the material. >> And I think this is a big fundamental challenge. So for instance for this customer they can um give privileges to their employees and have retrieval as acting on behalf like the system acts as the as the user and so you can match those same kind of like privileges for access. But if you start doing pre-training or mid-training on different parts it's like well if you pre-train on every piece of data uh that might only be accessible to say like the CEO of that company. So then you have to figure out how do you sort of bucket that knowledge and create different types of systems. Um but I think right now like we're after talking with the user they don't seem to have a great solution for sort of distilling all of the knowledge um in into like a single model or into a set of models. So like going you know going beyond retrieval to you know proper training and then I think also the supervised training they're doing is really akin to like the early days of chat GPT where it's like input output you have a you know a few examples and kind of transforming this new way of thinking was like no um highMP compute reinforcement learning is really effective this is how you should think about the strategies it's using this is how you create effective tool using towards those problems and this is how you optimize it effectively. >> Could you describe for folks who may not be familiar with it what you mean by mid-training because people are familiar with pre-training, they're familiar with post- training, but in the periodic context, what does mid training mean? >> Yeah, sorry for the lingo. So, I think this this term came up uh years ago where it's like, well, we had pre-training, we had post-training, but sometimes you need to put in a little bit more knowledge. So, uh before search worked really well, there was an issue of freshness. So we had pre-trained models and they have a knowledge cutoff. So there's like a scrape of the internet at that point, but users want more real-time knowledge. So it's like how do you get that in there and enter mid-rain? Midrain is basically you're taking new data, new knowledge that's not in the model and you continue pre-train. And this differs from standard post-training where post- training typically is more reinforcement learning, supervised um learning and the mechanism is basically or the the goal of it is just put a lot of knowledge into the model that doesn't exist before. >> Um so that's that's mid training in a nutshell. And in the periodic context, does that mean um essentially going and injecting a ton of custom sort of data from a an experimental implementation in a in a particular customer, a particular industry? What is the what are the sort of the the lines the atomic unit um that you guys think will of mid-training that will improve the capabilities of the models on on problems that they're just terrible at today? I mean it's just it's all all the knowledge. So it's like you can have very low-level descriptions of um physical objects. So like crystal structures for instance you can also have higher level semantic descriptions of like well this is how I made um material XYZ >> and trying to you know get all this data into the model um is really valuable. So it's like simulation data, experimental data, none of this exists. And basically putting that knowledge into the model and making sure that these distributions are connected in some way. And what I mean by that is if you just sort of mix together distribution A, B, and C, there's no guarantee of generalization. What you want to hope to see from these systems is the inclusion of this other data set is improving performance on the other data sets. And so these are sort of just like um machine learning techniques or machine learning problems to solve. Um but basically just make it an expert in physics in chemistry and where it was deficient before. >> Um you guys both know that I I uh spent some time running evals on a bunch of these models at the Stamford Physics Lab earlier this year and the results were that the models are terrible at scientific analysis >> because they weren't trained to do so >> because they weren't trained to do so. But on the other hand, you know, many of the the existing research teams working on the general models are investing in trying to make these better. Is there something about the way you're building periodic that gets to draft off of all of that progress in the base models or do you have to start everything from scratch and therefore not be able to be composable with advancements happening in the b the mainline models today? >> Yeah, I mean we benefit from a lot of different advances. So one of them is the LLMs are getting better. Um, and we definitely benefit from that because we take a pre-trained model and then mid-train it, you know, high computer. Another one is the physical simulation tools are getting better. They're open sourcing new ways of simulating, new ways of using machine learning to predict mh properties. So, we get to basically utilize all of those. Um, and it seems like machine learning has made such an impact in the physics and chemistry fields that we expect these improvements to continue. I think another thing is um when we think about tools for agents, we think of like here's a browser, here's a Python, but increasingly people think about tools as other neural nets as other agents. >> Um and so if you look at a lot of like physics code, it's not particularly deep. It's not this isn't competition programming. This is like kind of like hacky scripts but you can rely on uh some of the best systems for you know wherever they spike on. So neural net as a tool to these agents is something that immediately accelerates our work. Um so you don't have to like replicate every everything. There's a historical pattern that a lot of the fundamental research in the physical sciences that uh we're talking about here physics, chemistry, biology has historically been done at university labs. Um is there a role at all that the university ecosystem you think will play in periodic future or do you think these are just completely divergent paths? >> Absolutely. I mean so much of the simulation tooling we use have been developed in academia. um many of it is in Europe for example a lot of the novel synthesis methods so we definitely benefit from a lot of these different very deep technical progress uh like for example a lot of physical simulation tools are these you know complicated forrren code that in our team for example we don't really like know how to develop very efficiently but um we feel like there's definitely a very um deep connection between academia and industry labs So for example recently a lot of the large scale simulations have been done in industry labs like Microsoft deep mind and meta but a lot of those tools have been actually developed in academia and then passed on. So there's actually really nice synergy there. I think it add a few other things too. So like you found when you were evaluating models on their ability to do scientific analysis they were deficient. This was probably I mean not a direct goal for those teams training those models. So I think academia and these collaborations say will help us inform what are the important tasks like how do you do this analysis? What skills do we want to put in the model? Um a skill could be a full analysis or a skill could be like a a smaller primitive as part of a larger analysis but also secondarily it's how do you think? >> So one of the physicists uh was looking at the reasoning strategies of one of our models. He's like it's all wrong. It's all wrong. And we're like what do you mean? He's like, "No, this should be thinking higher level. It should be thinking in terms of symmetries." This is this is the book like that encodes like the thinking strategies that will be more effective. And of course, your reinforcement learning environment needs to reward those types of strategies. But given some of the most premier scientists are using these strategies, they're likely effective. And these are types of things where it's like an industry academic partnership can just be so powerful because industry just simply is blind to these types of analyses, these tools as well as just this way of thinking. >> Yeah. And there's a way of connecting that to the tool in question as well because you know language is very important but then in the human brain we also see a lot of visual processing like geometric. So it's plausible that while these LLMs will keep u getting better and better they'll actually benefit from having a geometric reasoning that's separate. So today we can do that with equariant graph neural networks. We can do it with diffusion models that are kind of geometric tools by construction and the LM can call them. So then it can have both the language aspect which is very good for say synthesis recipe but also the geometric aspects which is very good for representing atoms uh just design geometries in general. >> So how are you thinking about deepening periodic ties with academic lab? >> Yeah this is very important for us. So we have two major initiatives in this direction. One of them is we're starting an advisory board. Um this will be kind of expertise spanning from superc conductivity to solidst state chemistry to physics and we want to make sure you know we're in touch with this kind of long-term research directions. Um a lot of important government funding goes to these groups and we want to have a tight coupling between what's important for them and us. So uh this you know includes superc conductivity expertise such as ZX Chan from Stanford on the experimental side and Steve Kelsson from the theory side. Uh we also have census expertise on the advisory board from Mercury Canadas from uh Northwestern University and Chris Walverton on the high throughput DFT side. And our second initiative is going to be through a grant program. You know, we really want to enable some of this amazing work going on in academia and some of that work isn't a good fit for industry. You know, it's best done in academia. So we want to um kind of accept grant proposals and we want to enable and support the kind of work that's going to help community especially in relation to LLM's agents in synthesis materials discovery physics modeling. Uh so you know maybe after this show you can include the link. >> Yeah we'll include them in the show notes. So for people who might be interested in joining periodic what are you guys looking for? First off, someone deeply curious. Uh someone who really wants to understand the machine learning, the science at a deeper level, who wants to make contact with reality, who wants to advance science, like this has to be a driving thing. Um but also pragmatic. Um what we're trying to do is incredibly challenging and someone who has like very careful process and they get to they're solutionoriented, they get to goals quickly. um and really someone world class along some dimension. Um we're looking across all these different pillars. So machine learning, experimentalist, simulation and people who can bring some sort of innovation on what how do you create a creative ML system? Um how do you bring new types of tools or new types of thinking to some of these state-of-the-art models? um someone who can advance simulations and make it more robust and more reliable with experiment. >> Yeah. And maybe one more thing I'd add is Liam and I have been really looking for a sense of urgency in candidates because we want these technologies not in 10 years. You know, we don't want these LMS to start improving science in 10 years, but we want them ASAP. So if the candidate feels like a sense of urgency for improving these physical systems, uh discovering these amazing materials, innovating on superc conductivity, they would be a good fit. >> Yeah. If you match all these, please reach out. >> All right, sounds like we got to amp up the speed, the scale of stuff happening at Periodic. And we'll put the career links in the show notes. Thanks for coming, guys.